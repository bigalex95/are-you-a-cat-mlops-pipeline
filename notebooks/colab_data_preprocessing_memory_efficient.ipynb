{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ef3e544",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Cats vs Dogs - Memory Efficient Version\n",
    "\n",
    "This notebook uses **memory-efficient processing** to work within Colab's 12GB RAM limit.\n",
    "\n",
    "**Key improvements:**\n",
    "- ‚úÖ Process images in batches (not all at once)\n",
    "- ‚úÖ Use memory-mapped files to avoid loading everything into RAM\n",
    "- ‚úÖ Clear intermediate data after each step\n",
    "- ‚úÖ Monitor memory usage throughout\n",
    "\n",
    "**Run this notebook ONCE to prepare your data, then use the training notebook.**\n",
    "\n",
    "## What this notebook does:\n",
    "1. Downloads raw Cats vs Dogs dataset (~800MB)\n",
    "2. Preprocesses images **in batches** (resize to 150x150, normalize)\n",
    "3. Splits into train/val/test sets (70%/15%/15%)\n",
    "4. Saves processed data as memory-mapped .npy files\n",
    "5. Pushes to DVC remote for reuse\n",
    "\n",
    "**‚è±Ô∏è Expected time**: 15-20 minutes  \n",
    "**üíæ Peak RAM usage**: ~6-8 GB (vs 15+ GB in original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e728db",
   "metadata": {},
   "source": [
    "## 1. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3603f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your GitHub username and repo name\n",
    "GITHUB_USERNAME = \"bigalex95\"  # Change this to your username\n",
    "REPO_NAME = \"are-you-a-cat-mlops-pipeline\"\n",
    "REPO_URL = f\"https://github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
    "\n",
    "# Remove if already exists\n",
    "if os.path.exists(REPO_NAME):\n",
    "    !rm -rf {REPO_NAME}\n",
    "\n",
    "# Clone the repository\n",
    "!git clone {REPO_URL}\n",
    "\n",
    "# Change to repository directory\n",
    "%cd {REPO_NAME}\n",
    "\n",
    "print(\"\\n‚úÖ Repository cloned!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e575a8",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab16a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q tensorflow tensorflow-datasets\n",
    "!pip install -q dvc boto3 s3fs\n",
    "!pip install -q numpy pillow psutil\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa2d69c",
   "metadata": {},
   "source": [
    "## 3. Memory Monitoring Utility\n",
    "\n",
    "Track RAM usage throughout the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085a1f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import gc\n",
    "\n",
    "def print_memory_usage(label=\"\"):\n",
    "    \"\"\"Print current memory usage.\"\"\"\n",
    "    process = psutil.Process()\n",
    "    mem_info = process.memory_info()\n",
    "    mem_mb = mem_info.rss / 1024 / 1024\n",
    "    mem_gb = mem_mb / 1024\n",
    "    \n",
    "    # Get system memory\n",
    "    system_mem = psutil.virtual_memory()\n",
    "    system_total_gb = system_mem.total / 1024 / 1024 / 1024\n",
    "    system_used_gb = system_mem.used / 1024 / 1024 / 1024\n",
    "    system_percent = system_mem.percent\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìä Memory Usage {label}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Process Memory: {mem_gb:.2f} GB ({mem_mb:.0f} MB)\")\n",
    "    print(f\"System Memory: {system_used_gb:.2f}/{system_total_gb:.2f} GB ({system_percent:.1f}% used)\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "def clear_memory():\n",
    "    \"\"\"Force garbage collection to free memory.\"\"\"\n",
    "    gc.collect()\n",
    "    print(\"üßπ Cleared unused memory\")\n",
    "\n",
    "print_memory_usage(\"- Initial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf6b0d7",
   "metadata": {},
   "source": [
    "## 4. Configure DVC Remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762f6ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Option 1: Use Colab secrets (recommended)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    AWS_ACCESS_KEY_ID = userdata.get('AWS_ACCESS_KEY_ID')\n",
    "    AWS_SECRET_ACCESS_KEY = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
    "    print(\"‚úÖ Using credentials from Colab secrets\")\n",
    "except:\n",
    "    # Option 2: Enter credentials manually\n",
    "    print(\"Enter your DVC remote credentials (Backblaze B2 or S3):\")\n",
    "    AWS_ACCESS_KEY_ID = getpass(\"Access Key ID: \")\n",
    "    AWS_SECRET_ACCESS_KEY = getpass(\"Secret Access Key: \")\n",
    "\n",
    "# Set environment variables for DVC\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = AWS_ACCESS_KEY_ID\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = AWS_SECRET_ACCESS_KEY\n",
    "\n",
    "print(\"\\n‚úÖ DVC credentials configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be978569",
   "metadata": {},
   "source": [
    "## 5. Check Existing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eaf279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Check if processed data exists locally\n",
    "PROCESSED_DIR = 'data/processed'\n",
    "PROCESSED_FILES = [\n",
    "    'train_images.npy', 'train_labels.npy',\n",
    "    'val_images.npy', 'val_labels.npy',\n",
    "    'test_images.npy', 'test_labels.npy'\n",
    "]\n",
    "\n",
    "processed_exists_local = all(\n",
    "    os.path.exists(os.path.join(PROCESSED_DIR, f)) for f in PROCESSED_FILES\n",
    ")\n",
    "\n",
    "# Check if raw data exists locally\n",
    "RAW_DATA_DIR = 'data/raw/cats_vs_dogs/4.0.1'\n",
    "raw_exists_local = os.path.exists(RAW_DATA_DIR) and len(os.listdir(RAW_DATA_DIR)) > 0\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA STATUS CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if processed_exists_local:\n",
    "    print(\"‚úÖ Processed data exists locally\")\n",
    "    print(\"   You can skip to verification step\")\n",
    "elif raw_exists_local:\n",
    "    print(\"üìù Raw data exists, will process in batches\")\n",
    "else:\n",
    "    print(\"üì• Will download and process data in batches\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f1bc0c",
   "metadata": {},
   "source": [
    "## 6. Memory-Efficient Data Processing\n",
    "\n",
    "This cell processes the data in **batches** to avoid memory overload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c732bd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "print_memory_usage(\"- Before Processing\")\n",
    "\n",
    "if not processed_exists_local:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MEMORY-EFFICIENT DATA PROCESSING\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüìä Strategy:\")\n",
    "    print(\"  1. Load dataset info (without loading images)\")\n",
    "    print(\"  2. Pre-allocate memory-mapped output arrays\")\n",
    "    print(\"  3. Process images in small batches (500 at a time)\")\n",
    "    print(\"  4. Write directly to disk (no intermediate storage)\")\n",
    "    print(\"  5. Clear memory after each batch\\n\")\n",
    "    \n",
    "    # Configuration\n",
    "    TARGET_SIZE = (150, 150)\n",
    "    BATCH_SIZE = 500  # Process 500 images at a time\n",
    "    DATA_DIR = 'data/raw'\n",
    "    OUTPUT_DIR = 'data/processed'\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Step 1: Get dataset info without loading images\n",
    "    print(\"Step 1: Loading dataset metadata...\")\n",
    "    builder = tfds.builder('cats_vs_dogs', data_dir=DATA_DIR)\n",
    "    \n",
    "    # Download if needed (downloads but doesn't load into memory)\n",
    "    if not builder.downloaded:\n",
    "        print(\"  Downloading dataset (~800MB)...\")\n",
    "        builder.download_and_prepare()\n",
    "    \n",
    "    # Get total number of samples\n",
    "    info = builder.info\n",
    "    total_samples = info.splits['train'].num_examples\n",
    "    print(f\"  Total samples: {total_samples:,}\")\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    train_size = int(total_samples * 0.7)\n",
    "    val_size = int(total_samples * 0.15)\n",
    "    test_size = total_samples - train_size - val_size\n",
    "    \n",
    "    print(f\"  Train: {train_size:,} | Val: {val_size:,} | Test: {test_size:,}\")\n",
    "    \n",
    "    print_memory_usage(\"- After Loading Metadata\")\n",
    "    \n",
    "    # Step 2: Pre-allocate memory-mapped arrays\n",
    "    print(\"\\nStep 2: Pre-allocating output arrays (memory-mapped)...\")\n",
    "    \n",
    "    # Create memory-mapped arrays (stored on disk, accessed as if in RAM)\n",
    "    all_images_memmap = np.memmap(\n",
    "        os.path.join(OUTPUT_DIR, 'temp_all_images.npy'),\n",
    "        dtype='float32',\n",
    "        mode='w+',\n",
    "        shape=(total_samples, TARGET_SIZE[0], TARGET_SIZE[1], 3)\n",
    "    )\n",
    "    \n",
    "    all_labels_memmap = np.memmap(\n",
    "        os.path.join(OUTPUT_DIR, 'temp_all_labels.npy'),\n",
    "        dtype='int64',\n",
    "        mode='w+',\n",
    "        shape=(total_samples,)\n",
    "    )\n",
    "    \n",
    "    print(f\"  Created temp arrays: {total_samples:,} images √ó 150√ó150√ó3\")\n",
    "    print(f\"  Disk space used: ~{total_samples * 150 * 150 * 3 * 4 / 1024**3:.2f} GB\")\n",
    "    \n",
    "    print_memory_usage(\"- After Pre-allocation\")\n",
    "    \n",
    "    # Step 3: Load and process in batches\n",
    "    print(\"\\nStep 3: Processing images in batches...\")\n",
    "    \n",
    "    # Load dataset as iterator (doesn't load all into memory)\n",
    "    dataset = tfds.load(\n",
    "        'cats_vs_dogs',\n",
    "        split='train',\n",
    "        data_dir=DATA_DIR,\n",
    "        as_supervised=True,\n",
    "        shuffle_files=False  # We'll shuffle later\n",
    "    )\n",
    "    \n",
    "    # Process in batches\n",
    "    idx = 0\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    \n",
    "    with tqdm(total=total_samples, desc=\"Processing\") as pbar:\n",
    "        for image, label in tfds.as_numpy(dataset):\n",
    "            # Resize and normalize\n",
    "            img_resized = tf.image.resize(image, TARGET_SIZE).numpy()\n",
    "            if img_resized.max() > 1.0:\n",
    "                img_resized = img_resized / 255.0\n",
    "            \n",
    "            batch_images.append(img_resized)\n",
    "            batch_labels.append(label)\n",
    "            \n",
    "            # When batch is full, write to memmap and clear\n",
    "            if len(batch_images) >= BATCH_SIZE:\n",
    "                # Write batch to memory-mapped array\n",
    "                batch_start = idx\n",
    "                batch_end = idx + len(batch_images)\n",
    "                \n",
    "                all_images_memmap[batch_start:batch_end] = np.array(batch_images, dtype='float32')\n",
    "                all_labels_memmap[batch_start:batch_end] = np.array(batch_labels, dtype='int64')\n",
    "                \n",
    "                # Flush to disk\n",
    "                all_images_memmap.flush()\n",
    "                all_labels_memmap.flush()\n",
    "                \n",
    "                idx = batch_end\n",
    "                pbar.update(len(batch_images))\n",
    "                \n",
    "                # Clear batch\n",
    "                batch_images = []\n",
    "                batch_labels = []\n",
    "                clear_memory()\n",
    "        \n",
    "        # Write remaining images\n",
    "        if batch_images:\n",
    "            batch_start = idx\n",
    "            batch_end = idx + len(batch_images)\n",
    "            \n",
    "            all_images_memmap[batch_start:batch_end] = np.array(batch_images, dtype='float32')\n",
    "            all_labels_memmap[batch_start:batch_end] = np.array(batch_labels, dtype='int64')\n",
    "            \n",
    "            all_images_memmap.flush()\n",
    "            all_labels_memmap.flush()\n",
    "            \n",
    "            pbar.update(len(batch_images))\n",
    "    \n",
    "    print(\"\\n  ‚úÖ All images processed and written to disk\")\n",
    "    print_memory_usage(\"- After Processing\")\n",
    "    \n",
    "    # Step 4: Shuffle and split\n",
    "    print(\"\\nStep 4: Shuffling and splitting data...\")\n",
    "    \n",
    "    # Generate shuffled indices\n",
    "    np.random.seed(42)\n",
    "    indices = np.random.permutation(total_samples)\n",
    "    \n",
    "    # Split indices\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:train_size + val_size]\n",
    "    test_indices = indices[train_size + val_size:]\n",
    "    \n",
    "    print(f\"  Shuffled {total_samples:,} samples\")\n",
    "    \n",
    "    # Step 5: Save splits\n",
    "    print(\"\\nStep 5: Saving splits to final files...\")\n",
    "    \n",
    "    # Save training data\n",
    "    print(\"  Saving training data...\")\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'train_images.npy'), all_images_memmap[train_indices])\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'train_labels.npy'), all_labels_memmap[train_indices])\n",
    "    \n",
    "    # Save validation data\n",
    "    print(\"  Saving validation data...\")\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'val_images.npy'), all_images_memmap[val_indices])\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'val_labels.npy'), all_labels_memmap[val_indices])\n",
    "    \n",
    "    # Save test data\n",
    "    print(\"  Saving test data...\")\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'test_images.npy'), all_images_memmap[test_indices])\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'test_labels.npy'), all_labels_memmap[test_indices])\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    print(\"\\nStep 6: Cleaning up temporary files...\")\n",
    "    del all_images_memmap\n",
    "    del all_labels_memmap\n",
    "    clear_memory()\n",
    "    \n",
    "    os.remove(os.path.join(OUTPUT_DIR, 'temp_all_images.npy'))\n",
    "    os.remove(os.path.join(OUTPUT_DIR, 'temp_all_labels.npy'))\n",
    "    \n",
    "    print(\"  ‚úÖ Temporary files removed\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ DATA PROCESSING COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print_memory_usage(\"- Final\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Processed data already exists - skipping processing\")\n",
    "    print_memory_usage(\"- Current\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112110d0",
   "metadata": {},
   "source": [
    "## 7. Verify Processed Data\n",
    "\n",
    "Load data using memory-mapping to verify without using much RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a843594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Loading data with memory-mapping (efficient)...\\n\")\n",
    "\n",
    "# Load with memory-mapping (doesn't load into RAM)\n",
    "X_train = np.load('data/processed/train_images.npy', mmap_mode='r')\n",
    "y_train = np.load('data/processed/train_labels.npy', mmap_mode='r')\n",
    "\n",
    "X_val = np.load('data/processed/val_images.npy', mmap_mode='r')\n",
    "y_val = np.load('data/processed/val_labels.npy', mmap_mode='r')\n",
    "\n",
    "X_test = np.load('data/processed/test_images.npy', mmap_mode='r')\n",
    "y_test = np.load('data/processed/test_labels.npy', mmap_mode='r')\n",
    "\n",
    "print(\"Processed data files:\")\n",
    "!ls -lh data/processed/*.npy\n",
    "\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"  Training:   {X_train.shape} images, {y_train.shape} labels\")\n",
    "print(f\"  Validation: {X_val.shape} images, {y_val.shape} labels\")\n",
    "print(f\"  Test:       {X_test.shape} images, {y_test.shape} labels\")\n",
    "\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"  Training:   {np.sum(y_train == 0)} cats, {np.sum(y_train == 1)} dogs\")\n",
    "print(f\"  Validation: {np.sum(y_val == 0)} cats, {np.sum(y_val == 1)} dogs\")\n",
    "print(f\"  Test:       {np.sum(y_test == 0)} cats, {np.sum(y_test == 1)} dogs\")\n",
    "\n",
    "# Check a small sample\n",
    "sample = X_train[:10]\n",
    "print(f\"\\nValue ranges (sample):\")\n",
    "print(f\"  Min: {sample.min():.3f}\")\n",
    "print(f\"  Max: {sample.max():.3f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Data verification complete!\")\n",
    "print_memory_usage(\"- After Verification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa2a390",
   "metadata": {},
   "source": [
    "## 8. Visualize Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0174c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load just a few images for visualization\n",
    "sample_images = X_train[:10]\n",
    "sample_labels = y_train[:10]\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    axes[i].imshow(sample_images[i])\n",
    "    label = \"Dog\" if sample_labels[i] == 1 else \"Cat\"\n",
    "    axes[i].set_title(f\"{label}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Images look good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd685c2",
   "metadata": {},
   "source": [
    "## 9. Add to DVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178df6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists('data/processed.dvc'):\n",
    "    print(\"‚úÖ Data already tracked by DVC!\")\n",
    "    !dvc status data/processed.dvc\n",
    "else:\n",
    "    print(\"Adding processed data to DVC...\\n\")\n",
    "    !dvc add data/processed\n",
    "    print(\"\\n‚úÖ Data added to DVC!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4726c372",
   "metadata": {},
   "source": [
    "## 10. Push to DVC Remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pushing processed data to DVC remote...\")\n",
    "print(\"This may take a few minutes (~1-2GB upload)\\n\")\n",
    "\n",
    "!dvc push data/processed.dvc\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Processed data successfully pushed to DVC remote!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1584d2f",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### ‚úÖ What we accomplished:\n",
    "1. ‚úÖ Processed 23,000+ images in memory-efficient batches\n",
    "2. ‚úÖ Used memory-mapped files to avoid RAM overload\n",
    "3. ‚úÖ Split data into train/val/test sets\n",
    "4. ‚úÖ Tracked with DVC\n",
    "5. ‚úÖ Pushed to DVC remote\n",
    "\n",
    "### üí° Memory Optimizations Used:\n",
    "- **Batch Processing**: Processed 500 images at a time\n",
    "- **Memory-Mapped Files**: Arrays stored on disk, accessed like RAM\n",
    "- **Streaming**: Used TensorFlow datasets in iterator mode\n",
    "- **Garbage Collection**: Cleared memory after each batch\n",
    "\n",
    "### üìä Memory Comparison:\n",
    "- **Original Method**: 15+ GB peak RAM usage ‚ùå\n",
    "- **Optimized Method**: 6-8 GB peak RAM usage ‚úÖ\n",
    "\n",
    "### üéØ Next Steps:\n",
    "1. Use the `colab_model_training.ipynb` notebook to train your model\n",
    "2. Make sure to use memory-mapped loading or batch loading during training\n",
    "\n",
    "---\n",
    "\n",
    "**You're all set! Now use the training notebook with memory-efficient settings. üöÄ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
