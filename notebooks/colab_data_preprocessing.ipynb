{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "581a6fe5",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Cats vs Dogs - Google Colab\n",
    "\n",
    "This notebook downloads, preprocesses, and uploads the Cats vs Dogs dataset to DVC.\n",
    "\n",
    "**Run this notebook ONCE to prepare your data, then use the training notebook.**\n",
    "\n",
    "## What this notebook does:\n",
    "1. Downloads raw Cats vs Dogs dataset (~800MB)\n",
    "2. Preprocesses images (resize to 150x150, normalize)\n",
    "3. Splits into train/val/test sets (70%/15%/15%)\n",
    "4. Saves processed data as .npy files\n",
    "5. Pushes to DVC remote for reuse\n",
    "\n",
    "## Prerequisites:\n",
    "- Google account\n",
    "- Access to your GitHub repository\n",
    "- DVC remote credentials (Backblaze B2 or S3)\n",
    "\n",
    "**‚è±Ô∏è Expected time**: 15-20 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2802552f",
   "metadata": {},
   "source": [
    "## 1. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753fc161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your GitHub username and repo name\n",
    "GITHUB_USERNAME = \"bigalex95\"  # Change this to your username\n",
    "REPO_NAME = \"are-you-a-cat-mlops-pipeline\"\n",
    "REPO_URL = f\"https://github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
    "\n",
    "# Remove if already exists\n",
    "if os.path.exists(REPO_NAME):\n",
    "    !rm -rf {REPO_NAME}\n",
    "\n",
    "# Clone the repository\n",
    "!git clone {REPO_URL}\n",
    "\n",
    "# Change to repository directory\n",
    "%cd {REPO_NAME}\n",
    "\n",
    "print(\"\\n‚úÖ Repository cloned!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520ede76",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f57cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q tensorflow tensorflow-datasets\n",
    "!pip install -q dvc boto3 s3fs\n",
    "!pip install -q numpy pillow\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e187d15",
   "metadata": {},
   "source": [
    "## 3. Configure DVC Remote\n",
    "\n",
    "Set up your DVC credentials to push the processed data.\n",
    "\n",
    "**Security Note:** Use Colab secrets for credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9075edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Option 1: Use Colab secrets (recommended)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    AWS_ACCESS_KEY_ID = userdata.get('AWS_ACCESS_KEY_ID')\n",
    "    AWS_SECRET_ACCESS_KEY = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
    "    print(\"‚úÖ Using credentials from Colab secrets\")\n",
    "except:\n",
    "    # Option 2: Enter credentials manually\n",
    "    print(\"Enter your DVC remote credentials (Backblaze B2 or S3):\")\n",
    "    AWS_ACCESS_KEY_ID = getpass(\"Access Key ID: \")\n",
    "    AWS_SECRET_ACCESS_KEY = getpass(\"Secret Access Key: \")\n",
    "\n",
    "# Set environment variables for DVC\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = AWS_ACCESS_KEY_ID\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = AWS_SECRET_ACCESS_KEY\n",
    "\n",
    "print(\"\\n‚úÖ DVC credentials configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f9d858",
   "metadata": {},
   "source": [
    "## 4. Check Existing Data\n",
    "\n",
    "Let's check if we already have the data before downloading/processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997552f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Check if processed data exists locally\n",
    "PROCESSED_DIR = 'data/processed'\n",
    "PROCESSED_FILES = [\n",
    "    'train_images.npy', 'train_labels.npy',\n",
    "    'val_images.npy', 'val_labels.npy',\n",
    "    'test_images.npy', 'test_labels.npy'\n",
    "]\n",
    "\n",
    "processed_exists_local = all(\n",
    "    os.path.exists(os.path.join(PROCESSED_DIR, f)) for f in PROCESSED_FILES\n",
    ")\n",
    "\n",
    "# Check if raw data exists locally\n",
    "RAW_DATA_DIR = 'data/raw/cats_vs_dogs/4.0.1'\n",
    "raw_exists_local = os.path.exists(RAW_DATA_DIR) and len(os.listdir(RAW_DATA_DIR)) > 0\n",
    "\n",
    "# Check if data exists in DVC remote\n",
    "dvc_file_exists = os.path.exists('data/processed.dvc')\n",
    "processed_exists_remote = False\n",
    "\n",
    "if dvc_file_exists:\n",
    "    try:\n",
    "        # Check DVC status - if files are missing locally but exist in remote, status will show them\n",
    "        result = subprocess.run(\n",
    "            ['dvc', 'status', 'data/processed.dvc'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=False\n",
    "        )\n",
    "        # If status is empty or says \"Data and pipelines are up to date\", data exists in remote\n",
    "        output = result.stdout.strip()\n",
    "        if not output or 'up to date' in output.lower():\n",
    "            processed_exists_remote = True\n",
    "        elif 'not in cache' in output.lower():\n",
    "            processed_exists_remote = False\n",
    "        else:\n",
    "            # Try to check if we can pull (data exists in remote)\n",
    "            result = subprocess.run(\n",
    "                ['dvc', 'status', '--cloud', 'data/processed.dvc'],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                check=False\n",
    "            )\n",
    "            # If no output or \"up to date\", data is in remote\n",
    "            processed_exists_remote = not result.stdout.strip() or 'up to date' in result.stdout.lower()\n",
    "    except Exception as e:\n",
    "        print(f\"Note: Could not check DVC remote status: {e}\")\n",
    "        processed_exists_remote = False\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA STATUS CHECK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìÅ Raw Data Status (Local):\")\n",
    "if raw_exists_local:\n",
    "    print(\"  ‚úÖ Raw data exists in data/raw/\")\n",
    "    print(f\"  üìä Files found: {len(os.listdir(RAW_DATA_DIR))} files\")\n",
    "else:\n",
    "    print(\"  ‚ùå Raw data not found - will download (~800MB)\")\n",
    "\n",
    "print(f\"\\nüìÅ Processed Data Status:\")\n",
    "print(f\"\\n  Local:\")\n",
    "if processed_exists_local:\n",
    "    print(\"    ‚úÖ Processed data exists in data/processed/\")\n",
    "    # Show file sizes\n",
    "    !ls -lh data/processed/*.npy 2>/dev/null || echo \"    (Files exist but couldn't list)\"\n",
    "else:\n",
    "    print(\"    ‚ùå Processed data not found locally\")\n",
    "\n",
    "print(f\"\\n  DVC Remote:\")\n",
    "if dvc_file_exists:\n",
    "    if processed_exists_remote:\n",
    "        print(\"    ‚úÖ Processed data exists in DVC remote storage\")\n",
    "        if not processed_exists_local:\n",
    "            print(\"    üí° You can pull it with: dvc pull data/processed.dvc\")\n",
    "    else:\n",
    "        print(\"    ‚ùå Processed data not found in DVC remote\")\n",
    "else:\n",
    "    print(\"    ‚ö†Ô∏è  No DVC tracking file (data/processed.dvc) found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Decision flags\n",
    "NEED_DOWNLOAD = not raw_exists_local and not processed_exists_local\n",
    "NEED_PROCESS = not processed_exists_local\n",
    "CAN_PULL_FROM_DVC = dvc_file_exists and processed_exists_remote and not processed_exists_local\n",
    "\n",
    "if processed_exists_local:\n",
    "    print(\"üéâ Processed data exists locally! You can skip to step 8 to verify/push to DVC.\")\n",
    "elif CAN_PULL_FROM_DVC:\n",
    "    print(\"\udce5 Processed data exists in DVC remote but not locally.\")\n",
    "    print(\"   You can pull it (faster) or reprocess from raw data.\")\n",
    "    print(\"   Recommendation: Pull from DVC (run next cell to pull)\")\n",
    "elif not NEED_DOWNLOAD and NEED_PROCESS:\n",
    "    print(\"üìù Raw data exists locally, but needs processing.\")\n",
    "    print(\"   Will skip download and only preprocess.\")\n",
    "elif NEED_DOWNLOAD and NEED_PROCESS:\n",
    "    print(\"üì• Will download raw data and preprocess it.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Unusual state detected. Review the status above.\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d563f9",
   "metadata": {},
   "source": [
    "## 5. Pull from DVC Remote (if available)\n",
    "\n",
    "If processed data exists in DVC remote, pull it instead of reprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cd32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to pull from DVC if data exists in remote but not locally\n",
    "if CAN_PULL_FROM_DVC:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üì• PULLING PROCESSED DATA FROM DVC REMOTE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nThis is much faster than reprocessing!\")\n",
    "    print(\"Downloading processed data...\\n\")\n",
    "    \n",
    "    !dvc pull data/processed.dvc\n",
    "    \n",
    "    # Verify the pull was successful\n",
    "    if all(os.path.exists(os.path.join(PROCESSED_DIR, f)) for f in PROCESSED_FILES):\n",
    "        print(\"\\n‚úÖ Successfully pulled processed data from DVC remote!\")\n",
    "        processed_exists_local = True\n",
    "        NEED_PROCESS = False\n",
    "        NEED_DOWNLOAD = False\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Pull completed but some files are missing. Will need to reprocess.\")\n",
    "        NEED_PROCESS = True\n",
    "else:\n",
    "    if processed_exists_local:\n",
    "        print(\"‚è≠Ô∏è  Processed data already exists locally. Skipping DVC pull.\")\n",
    "    else:\n",
    "        print(\"‚è≠Ô∏è  No data in DVC remote. Will process from raw data or download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d52d0f8",
   "metadata": {},
   "source": [
    "## 6. Download and Preprocess Data (if needed)\n",
    "\n",
    "This will:\n",
    "- Download the Cats vs Dogs dataset (~800MB) - **only if raw data doesn't exist**\n",
    "- Resize all images to 150x150\n",
    "- Normalize pixel values to [0, 1]\n",
    "- Split into train (70%), validation (15%), test (15%)\n",
    "- Save as .npy files\n",
    "\n",
    "**‚è±Ô∏è This takes 10-15 minutes** (or skip if data exists locally or in DVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985af766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "# Only process if needed\n",
    "if NEED_PROCESS:\n",
    "    from data_loader import load_dataset\n",
    "    \n",
    "    print(\"Starting data preprocessing...\\n\")\n",
    "    print(\"This will:\")\n",
    "    if NEED_DOWNLOAD:\n",
    "        print(\"  1. Download raw Cats vs Dogs dataset (~800MB)\")\n",
    "    else:\n",
    "        print(\"  1. ‚úÖ Using existing raw data (skipping download)\")\n",
    "    print(\"  2. Preprocess and resize images to 150x150\")\n",
    "    print(\"  3. Split into train/val/test sets (70%/15%/15%)\")\n",
    "    print(\"  4. Save processed data as .npy files\")\n",
    "    print(\"\\n‚è±Ô∏è This may take 10-15 minutes...\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load, preprocess, split, and save data\n",
    "    # This function handles everything automatically\n",
    "    train_data, val_data, test_data = load_dataset(split='train')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"\\n‚úÖ Data preprocessing completed!\")\n",
    "else:\n",
    "    print(\"=\"*80)\n",
    "    print(\"‚è≠Ô∏è  SKIPPING: Processed data already exists!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nLoading existing processed data for verification...\")\n",
    "    \n",
    "    from preprocess import load_processed_data\n",
    "    train_data, val_data, test_data = load_processed_data()\n",
    "    \n",
    "    print(\"‚úÖ Existing data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d725422",
   "metadata": {},
   "source": [
    "## 7. Verify Processed Data\n",
    "\n",
    "Let's check what was created or loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b72877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Processed data files:\")\n",
    "!ls -lh data/processed/\n",
    "\n",
    "# Load and verify data shapes\n",
    "X_train, y_train = train_data\n",
    "X_val, y_val = val_data\n",
    "X_test, y_test = test_data\n",
    "\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"  Training:   {X_train.shape} images, {y_train.shape} labels\")\n",
    "print(f\"  Validation: {X_val.shape} images, {y_val.shape} labels\")\n",
    "print(f\"  Test:       {X_test.shape} images, {y_test.shape} labels\")\n",
    "\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"  Training:   {np.sum(y_train == 0)} cats, {np.sum(y_train == 1)} dogs\")\n",
    "print(f\"  Validation: {np.sum(y_val == 0)} cats, {np.sum(y_val == 1)} dogs\")\n",
    "print(f\"  Test:       {np.sum(y_test == 0)} cats, {np.sum(y_test == 1)} dogs\")\n",
    "\n",
    "print(f\"\\nValue ranges:\")\n",
    "print(f\"  Min: {X_train.min():.3f}\")\n",
    "print(f\"  Max: {X_train.max():.3f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Data verification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10945bef",
   "metadata": {},
   "source": [
    "## 8. Visualize Sample Images\n",
    "\n",
    "Let's verify the preprocessing worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71b932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize some training images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    axes[i].imshow(X_train[i])\n",
    "    label = \"Dog\" if y_train[i] == 1 else \"Cat\"\n",
    "    axes[i].set_title(f\"{label}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Images look good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9381564f",
   "metadata": {},
   "source": [
    "## 9. Add to DVC (if not already tracked)\n",
    "\n",
    "Track the processed data directory with DVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc3737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if already tracked by DVC\n",
    "if os.path.exists('data/processed.dvc'):\n",
    "    print(\"‚úÖ Data already tracked by DVC!\")\n",
    "    print(\"\\nExisting DVC file:\")\n",
    "    !ls -lh data/processed.dvc\n",
    "    \n",
    "    # Check if data changed\n",
    "    print(\"\\nChecking if data changed...\")\n",
    "    !dvc status data/processed.dvc\n",
    "else:\n",
    "    # Add processed data directory to DVC\n",
    "    print(\"Adding processed data to DVC...\\n\")\n",
    "    !dvc add data/processed\n",
    "    \n",
    "    print(\"\\n‚úÖ Data added to DVC!\")\n",
    "    print(\"\\nCreated files:\")\n",
    "    !ls -lh data/processed.dvc data/.gitignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f595b1",
   "metadata": {},
   "source": [
    "## 10. Push to DVC Remote\n",
    "\n",
    "Upload the processed data to your remote storage (if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0608e772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we need to push to remote\n",
    "if processed_exists_remote and not NEED_PROCESS:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úÖ Data already exists in DVC remote - skipping push\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"Pushing processed data to DVC remote...\")\n",
    "    print(\"This may take a few minutes (~1-2GB upload)\\n\")\n",
    "    \n",
    "    !dvc push data/processed.dvc\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ Processed data successfully pushed to DVC remote!\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5126c1a4",
   "metadata": {},
   "source": [
    "## 11. Commit to Git\n",
    "\n",
    "Commit the DVC metadata files to your repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f22b818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure git (replace with your info)\n",
    "!git config --global user.email \"your.email@example.com\"\n",
    "!git config --global user.name \"Your Name\"\n",
    "\n",
    "# Add DVC files\n",
    "!git add data/processed.dvc data/.gitignore\n",
    "\n",
    "# Commit changes\n",
    "!git commit -m \"Add preprocessed data to DVC\"\n",
    "\n",
    "print(\"\\n‚úÖ Changes committed!\")\n",
    "print(\"\\nTo push to GitHub, authenticate and run:\")\n",
    "print(\"  git push origin model-development\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32af3393",
   "metadata": {},
   "source": [
    "## 12. Summary\n",
    "\n",
    "### ‚úÖ What we accomplished:\n",
    "1. ‚úÖ Checked for existing data (local and DVC remote)\n",
    "2. ‚úÖ Pulled from DVC remote (if available) OR\n",
    "3. ‚úÖ Downloaded Cats vs Dogs dataset (if needed) OR\n",
    "4. ‚úÖ Preprocessed images (if needed)\n",
    "5. ‚úÖ Verified data quality\n",
    "6. ‚úÖ Tracked with DVC (if not already)\n",
    "7. ‚úÖ Pushed to DVC remote (if needed)\n",
    "8. ‚úÖ Committed metadata to git\n",
    "\n",
    "### üìä Processed Data:\n",
    "- **Location**: `data/processed/`\n",
    "- **Files**: train_images.npy, train_labels.npy, val_images.npy, val_labels.npy, test_images.npy, test_labels.npy\n",
    "- **DVC file**: `data/processed.dvc`\n",
    "\n",
    "### üéØ Next Steps:\n",
    "1. Push changes to GitHub (if not done automatically)\n",
    "2. **Use the `colab_model_training.ipynb` notebook to train your model**\n",
    "3. The training notebook will pull this preprocessed data automatically\n",
    "\n",
    "### üí° Smart Features:\n",
    "- ‚ú® **Checks DVC remote** before downloading/processing\n",
    "- ‚ú® **Pulls from DVC** if data exists (much faster than reprocessing)\n",
    "- ‚ú® **Skips unnecessary steps** automatically\n",
    "- ‚ú® **Handles all scenarios**:\n",
    "  - Data exists in DVC remote ‚Üí Pull it\n",
    "  - Data exists locally ‚Üí Skip everything\n",
    "  - Raw data exists ‚Üí Skip download, only preprocess\n",
    "  - Nothing exists ‚Üí Download and preprocess\n",
    "\n",
    "### üìä Typical Scenarios:\n",
    "\n",
    "**First Time (Nothing exists):**\n",
    "- ‚è±Ô∏è 15-20 minutes: Download + Preprocess + Push\n",
    "\n",
    "**Second Time (Data in DVC):**\n",
    "- ‚è±Ô∏è 2-3 minutes: Pull from DVC\n",
    "\n",
    "**Local Data Exists:**\n",
    "- ‚è±Ô∏è 30 seconds: Verify and update DVC if needed\n",
    "\n",
    "---\n",
    "\n",
    "**You're all set! Now use the training notebook to train your model. üöÄ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
