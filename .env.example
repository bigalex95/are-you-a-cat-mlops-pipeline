# ============================================================================
# Environment Configuration Template
# ============================================================================
# Project: are-you-a-cat-mlops-pipeline
# Description: MLOps pipeline for cat vs dog image classification
# 
# IMPORTANT: 
#   1. Copy this file to .env and fill in actual values
#   2. NEVER commit .env file to version control
#   3. Keep credentials secure and rotate them regularly
# ============================================================================

# ============================================================================
# APPLICATION SERVICES - Ports Configuration
# ============================================================================
# Ports for various services in docker-compose
STREAMLIT_PORT=8501          # Streamlit web app UI
MLFLOW_PORT=5000             # MLflow tracking server
AIRFLOW_PORT=8080            # Airflow web interface
POSTGRES_PORT=5432           # PostgreSQL database

# ============================================================================
# POSTGRESQL DATABASE - Airflow Metadata Store
# ============================================================================
# Database credentials for Airflow's metadata backend
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow    # CHANGE IN PRODUCTION!
POSTGRES_DB=airflow

# ============================================================================
# APACHE AIRFLOW - Workflow Orchestration
# ============================================================================

# --- User/Group Configuration ---
# Stable UID/GID for proper file permissions on Linux hosts
AIRFLOW_UID=50000
AIRFLOW_GID=0

# --- Security Keys ---
# CRITICAL: Generate strong keys for production deployments!

# Fernet Key: Used for encrypting sensitive data in Airflow metadata
# Generate new key: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
AIRFLOW__CORE__FERNET_KEY=

# Webserver Secret Key: Used for session management and CSRF protection
# Generate random string: openssl rand -hex 32
AIRFLOW__WEBSERVER__SECRET_KEY=

# ============================================================================
# BACKBLAZE B2 - Cloud Storage (DVC Remote)
# ============================================================================
# Backblaze B2 credentials for data versioning with DVC
# Find these in your B2 console under 'App Keys'

B2_BUCKET=                   # Your B2 bucket name (e.g., my-mlops-data)
B2_KEY_ID=                   # B2 Key ID / Access Key
B2_APPLICATION_KEY=          # B2 Application Key / Secret (keep secure!)

# B2 S3-compatible endpoint
# Examples:
#   - s3.us-west-004.backblazeb2.com
#   - s3.eu-central-003.backblazeb2.com
B2_ENDPOINT=https://s3.us-west-004.backblazeb2.com

# Region extracted from endpoint (e.g., us-west-004, eu-central-003)
B2_REGION=us-west-004

# ============================================================================
# AWS S3 COMPATIBILITY - For boto3/DVC Integration
# ============================================================================
# AWS-compatible environment variables that map to B2 credentials
# Used by boto3, DVC, and other S3-compatible tools

AWS_ACCESS_KEY_ID=           # Same as B2_KEY_ID
AWS_SECRET_ACCESS_KEY=       # Same as B2_APPLICATION_KEY
AWS_ENDPOINT_URL=            # Same as B2_ENDPOINT
AWS_DEFAULT_REGION=          # Same as B2_REGION

# ============================================================================
# MLFLOW - Experiment Tracking & Model Registry
# ============================================================================

# MLflow tracking server URI
# Default for local docker-compose: http://mlflow-server:5000
# For external access: http://localhost:5000
MLFLOW_TRACKING_URI=http://mlflow-server:5000

# S3-compatible endpoint for MLflow artifact storage (optional)
# Set to B2_ENDPOINT if storing artifacts in B2
MLFLOW_S3_ENDPOINT_URL=

# ============================================================================
# DVC - Data Version Control
# ============================================================================

# Default DVC remote name
# Should match the remote configured in .dvc/config
DVC_REMOTE=myremote

# ============================================================================
# DAGSHUB - Collaboration Platform (Optional)
# ============================================================================
# DagsHub token for MLflow tracking and DVC remote
# Get your token from: https://dagshub.com/user/settings/tokens

DAGSHUB_TOKEN=dghp_YOUR_TOKEN_HERE

# ============================================================================
# NOTES & BEST PRACTICES
# ============================================================================
#
# ðŸ“‹ Setup Instructions:
#   1. Copy this file: cp .env.example .env
#   2. Fill in your actual credentials in .env
#   3. For B2 setup automation: python scripts/setup_b2_dvc.py --interactive
#
# ðŸ”’ Security:
#   - Use strong, unique passwords for production
#   - Rotate credentials regularly
#   - Never commit .env to git (check .gitignore)
#   - Use secrets management for production (e.g., AWS Secrets Manager)
#
# ðŸ”§ Testing:
#   - Test B2 connection: ./scripts/dvc_helper.sh check
#   - Verify DVC setup: dvc remote list
#   - Check Airflow: docker-compose up airflow-webserver
#
# ðŸ“š Documentation:
#   - B2 Setup: See scripts/setup_b2_dvc.py
#   - DVC Docs: https://dvc.org/doc
#   - Airflow Docs: https://airflow.apache.org/docs/
#   - MLflow Docs: https://mlflow.org/docs/latest/index.html
#
# ============================================================================